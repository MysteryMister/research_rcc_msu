{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175af794-ae95-421a-8bdb-bd32529ca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import sentenize\n",
    "\n",
    "import pymorphy2 as pm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa7325dd-4914-4aa9-8ddb-cfc4e7b5f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_cache_token_parse(lemmatizer, token, cache):\n",
    "    \"\"\"Extracts token parse from cache, or adds it to the latter if not found.\"\"\"\n",
    "    if token in cache:\n",
    "        token_parse = cache[token]\n",
    "    else:\n",
    "        token_parse = lemmatizer.parse(token)[0]\n",
    "        cache[token] = token_parse\n",
    "\n",
    "    return token_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dacd7e8-f05a-4098-9581-c511e34343d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = pm2.MorphAnalyzer(lang='ru')\n",
    "pm2_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628aebe0-a6fd-4c5e-85ab-1099c39463a9",
   "metadata": {},
   "source": [
    "Загрузим датасет - корпус научных статей по физике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94857d2c-0ae1-4d8a-b880-f106af7b6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ffd71fc51042ee85310b2d7f78c63c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса: 15000 статей\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../data/elibrary_physics_15k'\n",
    "article_id = re.compile(r'_([0-9]+).htm$')\n",
    "\n",
    "file_contents = {}\n",
    "for filename in tqdm(os.scandir(data_directory), desc='Loading data'):\n",
    "    if filename.is_file():\n",
    "        file_name = filename.name\n",
    "        file_id = int(article_id.search(file_name).groups(1)[0])\n",
    "        \n",
    "        with open(filename, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            file_contents[file_id] = text\n",
    "\n",
    "print(f'Размер корпуса: {len(file_contents)} статей')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6c321-629b-4b4c-b26d-f693ede15c8f",
   "metadata": {},
   "source": [
    "Удалим из корпуса статьи, написанные не на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3426ab9e-41b6-4329-88fa-a1f9b0f33b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CYRILLIC_THRESHOLD = 0.2\n",
    "\n",
    "cyrillic_sym = re.compile(r'[а-яёА-ЯЁ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff8573-f101-4cbe-ab64-214ec5029965",
   "metadata": {},
   "source": [
    "Алгоритм отсеивания статей по языку:\n",
    "1. подсчитывается кол-во кириллических символов в статье;\n",
    "2. если соотношение кириллицы >= MIN_CYRILLIC_THRESHOLD -> статья русскоязычная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977fdf23-19e9-4aff-aea4-bec94b161115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56255153048d4895b8bd359686f21d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering out non-russian articles:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено нерусскоязычных статей: 4062\n"
     ]
    }
   ],
   "source": [
    "rus_articles = {}\n",
    "\n",
    "for id, article in tqdm(file_contents.items(), desc='Filtering out non-russian articles'):\n",
    "    rus_count = 0\n",
    "    for _ in cyrillic_sym.finditer(article):\n",
    "        rus_count += 1\n",
    "\n",
    "    if rus_count >= len(article) * MIN_CYRILLIC_THRESHOLD:\n",
    "        rus_articles[id] = article\n",
    "\n",
    "print(f'Удалено нерусскоязычных статей: {len(file_contents) - len(rus_articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "867cdd16-eb95-4206-a8ae-09a8abcd2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во русскоязычных статей: 10938\n"
     ]
    }
   ],
   "source": [
    "articles_count = len(rus_articles)\n",
    "print(f'Кол-во русскоязычных статей: {articles_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bef87f-11fb-4403-8b69-d08e3575d556",
   "metadata": {},
   "source": [
    "#### 1.2 Arranging lemmatizing procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602621e1-e760-42d7-be73-1fd76c6a4080",
   "metadata": {},
   "source": [
    "Напишем вспомогательные функции для анализа лемм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0face676-40c7-49f4-951f-4b6e4de2c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bigram(word1_parse, word2_parse):\n",
    "    \"\"\"\n",
    "    Tries to match bigram of w1 and w2 with any of the patterns.\n",
    "    If succeed, returns a tuple ((w1_norm, w2_norm), pattern).\n",
    "    \"\"\"\n",
    "    word1_pos = word1_parse.tag.POS\n",
    "    word2_pos = word2_parse.tag.POS\n",
    "\n",
    "    # failed pattern matching\n",
    "    if word2_pos != 'NOUN' or word1_pos not in {'NOUN', 'ADJF'}:\n",
    "        return None\n",
    "\n",
    "    word1_inflection = (word1_parse.tag.gender, word1_parse.tag.number, word1_parse.tag.case)\n",
    "    word2_inflection = (word2_parse.tag.gender, word2_parse.tag.number, word2_parse.tag.case)\n",
    "\n",
    "    # 'adj + noun' pattern\n",
    "    if (\n",
    "        word1_pos == 'ADJF' and word2_pos == 'NOUN' \n",
    "        and word1_inflection[1] == word2_inflection[1]\n",
    "        and word1_inflection[2] == word2_inflection[2]\n",
    "        and (\n",
    "            word1_inflection[0] == word2_inflection[0]\n",
    "            or word1_inflection[0] is None and word1_inflection[1] == 'plur'\n",
    "        )\n",
    "    ):\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'прил. + сущ.'\n",
    "\n",
    "    # 'noun + noun' pattern\n",
    "    if word1_pos == 'NOUN' and word2_pos == 'NOUN' and word2_inflection[2] == 'gent':\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'сущ. + сущ.'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2413864d-4e4a-4b3c-a13d-293362fe44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_bigram(bigram, lemmatizer, cache):\n",
    "    \"\"\"Simplifies bigram and returns its normal form.\"\"\"\n",
    "    unigram_1, unigram_2 = tuple(bigram.split(sep='_'))\n",
    "\n",
    "    normgram_1 = get_or_cache_token_parse(lemmatizer, unigram_1, cache).normal_form\n",
    "    normgram_2 = get_or_cache_token_parse(lemmatizer, unigram_2, cache).normal_form\n",
    "\n",
    "    return f'{normgram_1}_{normgram_2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafe8ec-9459-482a-97f4-65f4d82c10df",
   "metadata": {},
   "source": [
    "Зададим список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49130ec8-1288-4d9c-a287-3da7e9dd84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "stop_words.extend(\n",
    "    [\n",
    "        # adjective-like pronouns\n",
    "        'мой', 'твой', 'ваш', 'наш', 'свой', 'его', 'ее', 'их',\n",
    "        'тот', 'этот', 'такой', 'таков', 'сей', 'который',\n",
    "        'весь', 'всякий', 'сам', 'самый', 'каждый', 'любой', 'иной', 'другой',\n",
    "        'какой', 'каков', 'чей', 'никакой', 'ничей',\n",
    "        'какой-то', 'какой-либо', 'какой-нибудь', 'некоторый', 'некий',\n",
    "        # participles\n",
    "        'соответствующий', 'следующий', 'данный',\n",
    "        # numerals\n",
    "        'один',\n",
    "        # insignificant words\n",
    "        'друг',\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5c33b-abdd-4983-b0cc-536a2c9056bc",
   "metadata": {},
   "source": [
    "Зададим список стоп-биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff60d398-f697-498f-9af0-383f26e0c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bigrams = []\n",
    "\n",
    "stop_bigrams.extend(\n",
    "    [\n",
    "        ('крайний', 'мера'), ('сегодняшний', 'день'), ('настоящий', 'время'), ('настоящий', 'работа'),\n",
    "        ('настоящий', 'статья'), ('точка', 'зрение'), ('первый', 'очередь'), ('последний', 'год'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_bigrams = set(stop_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec192fe4-f110-4f17-ab0a-3adf669cbdf7",
   "metadata": {},
   "source": [
    "Подходящие слова должны содержать только буквы и дефисы (последние не в начале/конце слова):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7915a328-0c4e-4901-ba41-ded7f1fb2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_unigram(word):\n",
    "    \"\"\"Checks if a word is long enough and contains only letters and dashes.\"\"\"\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        return False\n",
    "\n",
    "    word_dashless = word.replace('-', '')\n",
    "    return len(word_dashless) >= 3 and word_dashless.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ff3d9-9c73-457d-bfc6-377cc36b7bdb",
   "metadata": {},
   "source": [
    "Загрузим датасет предложений с посчитанными метриками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc1d3d4-fb2c-42bc-9484-05b53ceaa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mi3_df = pd.read_csv('../data/data_frames/bigram_tfidf_mi3_scores_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2c78b19-bdc7-439b-a371-94b7f9a4344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_predicate(lemmatizer, cache, tokens, predicates):\n",
    "    \"\"\"Checks if the sentence contains a predicate.\"\"\"\n",
    "    for token in tokens:\n",
    "        token_pos = get_or_cache_token_parse(lemmatizer, token, cache).tag.POS\n",
    "        if token_pos in predicates:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1762b08-ce14-4a10-a47e-168769bc9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_hyphen(tokens):\n",
    "    \"\"\"Checks if the sentence has a hyphen and is valid.\"\"\"\n",
    "    valid_tokens_count = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) > 2:\n",
    "            valid_tokens_count += 1\n",
    "    if valid_tokens_count < 3:\n",
    "        return False\n",
    "\n",
    "    hyphens = ('-', '−', '–', '—')\n",
    "    for x in hyphens:\n",
    "        if x in tokens:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472be70-1453-4db2-b880-7382d6b53816",
   "metadata": {},
   "source": [
    "Наложим ограничения на попадание в выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ed0d907-18a5-449a-a5b0-d9c95d9aa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suitability_status(sentence, formulae_pattern, external_link, internal_link, good_symbols):\n",
    "    \"\"\"Checks all conditions to include the sentence in the dataset.\"\"\"\n",
    "    if (\n",
    "        # no equations\n",
    "        formulae_pattern.search(sentence) is None\n",
    "        # no external links\n",
    "        and external_link.search(sentence) is None\n",
    "        # no internal links\n",
    "        and internal_link.search(sentence) is None\n",
    "        # no incomplete fragments\n",
    "        and len(sentence) >= 10\n",
    "    ):\n",
    "        bad_symbols = set(sentence.lower()) - good_symbols\n",
    "\n",
    "        if not bad_symbols:\n",
    "            # remove the mess in the beginning\n",
    "            first_letter = re.search(r'[А-ЯA-Z]', sentence)\n",
    "            if first_letter is not None:\n",
    "                return first_letter.start()\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84831de-3279-4292-b9a5-95f2a57bf8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_pattern = re.compile('(==.*==(\\\\n)+)+')\n",
    "clumped_pattern = re.compile('[а-яА-Я0-9]\\.[а-яА-Я0-9]')\n",
    "\n",
    "formulae_pattern = re.compile(r'\\n.\\n.\\n')\n",
    "external_link_pattern = re.compile(r'\\[([0-9])+\\]')\n",
    "internal_link_pattern = re.compile(r'\\(([0-9])+\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a158e518-437c-4b0c-8053-1d087707fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = {'INFN', 'VERB', 'PRTS', 'ADJS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62338ce0-ce24-4134-acc1-00e6df60ab92",
   "metadata": {},
   "source": [
    "Отберем все символы, не встречающиеся в формулах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2ff88f3-26a6-4472-bf18-b962cfb540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_symbols = {\n",
    "    'а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', \n",
    "    'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "    's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    '(', ')', '%', '\\n', ' ', ',', '.', '\"', '\\'', '!', '/', ':', ';', '?', '[', ']', '«', '»',\n",
    "    '³', '—', '–', '−', '-', '…', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf523eb-37f4-48aa-94e7-6ee6df94be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text, text_idx, bigram):\n",
    "    \"\"\"Extracts classifiable sentences for target bigram from text.\"\"\"\n",
    "    sentences = []\n",
    "    for idx, substr in enumerate(sentenize(text)):\n",
    "        sentence = substr.text.strip(r'\\n')\n",
    "        \n",
    "        # removing subtitles\n",
    "        subtitle = subtitle_pattern.search(sentence)\n",
    "        if subtitle is not None:\n",
    "            sentence = sentence[:subtitle.start(0)] + sentence[subtitle.end(0):]\n",
    "        \n",
    "        # capital letter check\n",
    "        if not sentence[0].isupper():\n",
    "            continue\n",
    "        \n",
    "        # removing sentences with newline characters\n",
    "        if sentence.find('\\n') != -1:\n",
    "            continue\n",
    "\n",
    "        # removing clumped sentences\n",
    "        if clumped_pattern.search(sentence) is not None:\n",
    "            continue\n",
    "\n",
    "        # deemphasize sentence\n",
    "        sentence = sentence.replace('́', '')\n",
    "        \n",
    "        tokens = word_tokenize(sentence.lower(), language='russian')\n",
    "\n",
    "        # saving only sentences with target bigram\n",
    "        has_target_bigram = False\n",
    "        normalized_tokens = [get_or_cache_token_parse(lemmatizer, x, pm2_cache).normal_form for x in tokens]\n",
    "        for i in range(len(normalized_tokens) - 1):\n",
    "            if normalized_tokens[i] == bigram[0] and normalized_tokens[i + 1] == bigram[1]:\n",
    "                has_target_bigram = True\n",
    "                break\n",
    "        if not has_target_bigram:\n",
    "            continue\n",
    "        \n",
    "        # removing sentences without predicates\n",
    "        has_predicates = True\n",
    "        \n",
    "        standardized_tokens = []\n",
    "        for token in tokens:\n",
    "            if match_unigram(token):\n",
    "                standardized_tokens.append(token)\n",
    "\n",
    "        if (\n",
    "            not has_hyphen(tokens)\n",
    "            and not has_predicate(lemmatizer, pm2_cache, standardized_tokens, predicates)\n",
    "        ):\n",
    "            has_predicates = False\n",
    "\n",
    "        # removing formulas and links\n",
    "        sentence_start = get_suitability_status(sentence, formulae_pattern, external_link_pattern, internal_link_pattern, good_symbols)\n",
    "        if sentence_start != -1 and has_predicates:\n",
    "            sentences.append([sentence[sentence_start:], text_idx])\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3e3fd-3f44-43ea-bd26-d2d4a5442e4f",
   "metadata": {},
   "source": [
    "Сохраним подходящие контексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c678088e-2ebf-41cf-ba17-14ce470d8f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_1 = bigram_mi3_df[(bigram_mi3_df['unigram_1'] == 'электромагнитный') & (bigram_mi3_df['unigram_2'] == 'волна')]['articles_id_list']\n",
    "article_1 = eval(list(article_1.values)[0])\n",
    "\n",
    "article_2 = bigram_mi3_df[(bigram_mi3_df['unigram_1'] == 'акустический') & (bigram_mi3_df['unigram_2'] == 'эмиссия')]['articles_id_list']\n",
    "article_2 = eval(list(article_2.values)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e70c0c08-5ae4-4ed5-9759-8f3959b940ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_1 = []\n",
    "\n",
    "for article_id in article_1:\n",
    "    text = rus_articles[article_id]\n",
    "    matched_1.extend(extract_sentences(text, article_id, ('электромагнитный', 'волна')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c93021ea-b155-4d24-adda-27de100c9c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_2 = []\n",
    "\n",
    "for article_id in article_2:\n",
    "    text = rus_articles[article_id]\n",
    "    matched_2.extend(extract_sentences(text, article_id, ('акустический', 'эмиссия')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1fcc6d0-19b4-47c6-9f74-bbfe912c544c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Электромагнитные волны в неоднородной гиротроп...</td>\n",
       "      <td>10007924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ВВЕДЕНИЕ Как известно (см., например, монограф...</td>\n",
       "      <td>10008260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R диэлектрической проницаемости, но не равные ...</td>\n",
       "      <td>10008260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Исходя из этого, можно сделать заключение, что...</td>\n",
       "      <td>10008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Домены тока здесь возникают из-за взаимодейств...</td>\n",
       "      <td>10008529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>Магноны - это квазичастицы спиновых волн, как ...</td>\n",
       "      <td>9976930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>Исходя из уравнения движения электронов в поле...</td>\n",
       "      <td>9989558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>780</th>\n",
       "      <td>Электромагнитная волна при этом полностью отда...</td>\n",
       "      <td>9989558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>Высокие уровни напряжения (сотни киловольт) и ...</td>\n",
       "      <td>9989725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>Для расчета обострения фронта импульса напряже...</td>\n",
       "      <td>9989725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>783 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  article_id\n",
       "0    Электромагнитные волны в неоднородной гиротроп...    10007924\n",
       "1    ВВЕДЕНИЕ Как известно (см., например, монограф...    10008260\n",
       "2    R диэлектрической проницаемости, но не равные ...    10008260\n",
       "3    Исходя из этого, можно сделать заключение, что...    10008529\n",
       "4    Домены тока здесь возникают из-за взаимодейств...    10008529\n",
       "..                                                 ...         ...\n",
       "778  Магноны - это квазичастицы спиновых волн, как ...     9976930\n",
       "779  Исходя из уравнения движения электронов в поле...     9989558\n",
       "780  Электромагнитная волна при этом полностью отда...     9989558\n",
       "781  Высокие уровни напряжения (сотни киловольт) и ...     9989725\n",
       "782  Для расчета обострения фронта импульса напряже...     9989725\n",
       "\n",
       "[783 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(matched_1, columns=['sentence', 'article_id'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4fff80d-1b7a-43d8-ac03-afa3ac406a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Проведение циклов мартенситных превращений в у...</td>\n",
       "      <td>10007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Характерным процессом при циклировании мартенс...</td>\n",
       "      <td>10007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Остается невыясненным характер акустической эм...</td>\n",
       "      <td>10007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>В термомеханическом цикле в одном и том же вре...</td>\n",
       "      <td>10007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Результаты экспериментов и их обсуждение Резул...</td>\n",
       "      <td>10007974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Исследования показали, что наноиндентирование ...</td>\n",
       "      <td>9934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Для измерений акустической эмиссии использовал...</td>\n",
       "      <td>9934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Эксперименты показали, что увеличение нагрузки...</td>\n",
       "      <td>9934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Результаты измерения твердости, модуля Юнга, а...</td>\n",
       "      <td>9934683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Поскольку метод математического моделирования ...</td>\n",
       "      <td>9976731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              sentence  article_id\n",
       "0    Проведение циклов мартенситных превращений в у...    10007974\n",
       "1    Характерным процессом при циклировании мартенс...    10007974\n",
       "2    Остается невыясненным характер акустической эм...    10007974\n",
       "3    В термомеханическом цикле в одном и том же вре...    10007974\n",
       "4    Результаты экспериментов и их обсуждение Резул...    10007974\n",
       "..                                                 ...         ...\n",
       "171  Исследования показали, что наноиндентирование ...     9934683\n",
       "172  Для измерений акустической эмиссии использовал...     9934683\n",
       "173  Эксперименты показали, что увеличение нагрузки...     9934683\n",
       "174  Результаты измерения твердости, модуля Юнга, а...     9934683\n",
       "175  Поскольку метод математического моделирования ...     9976731\n",
       "\n",
       "[176 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(matched_2, columns=['sentence', 'article_id'])\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c740482b-07c7-451a-8fe1-9ca46c502de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('../data/data_frames/data_for_eval/электромагнитный_волна.csv')\n",
    "df2.to_csv('../data/data_frames/data_for_eval/акустический_эмиссия.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468373a7-460b-4ccb-ad24-68d1eb9e21b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

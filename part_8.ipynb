{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e55a23da-814e-455d-baaa-2a6d76d694d7",
   "metadata": {},
   "source": [
    "# Sentence Labeling Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80b5389-061b-45b7-a683-f7c34d6256c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 as pm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba6ef0-80bc-4aff-b23c-3d88b3d84faf",
   "metadata": {},
   "source": [
    "### 0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d060a3e1-6e7f-4079-85c6-e61e733a9ae8",
   "metadata": {},
   "source": [
    "Создадим обучающую выборку для задачи классификации предложений на возможные/невозможные в генерируемых выжимках по терминам."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead581fa-6a15-43f6-88c9-eefdca5d03e0",
   "metadata": {},
   "source": [
    "Выборка будет состоять из предложений для 10 самых частотных биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ed8dae-bcd1-4afd-9123-616aff2daf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_wikigrams = {\n",
    "    'длина_волна',\n",
    "    'система_уравнение',\n",
    "    'магнитный_поле',\n",
    "    'электрический_поле',\n",
    "    'момент_время',\n",
    "    'решение_уравнение',\n",
    "    'система_координата',\n",
    "    'решение_задача',\n",
    "    'комнатный_температура',\n",
    "    'твёрдый_тело',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6001ec45-8976-4fee-bf7f-def66c6ed231",
   "metadata": {},
   "source": [
    "Впоследствии можно будет взять коллекцию из большего числа биграмм."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0022a558-cef4-4d5b-b3d9-2f93a6bbb404",
   "metadata": {},
   "source": [
    "### 1. Necessary Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b427cbfd-29bc-4c44-a0c1-3a5716947885",
   "metadata": {},
   "source": [
    "#### 1.1 Loading corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa7730-f980-4a2e-aefd-873c5bb7d43a",
   "metadata": {},
   "source": [
    "Загрузим датасет - корпус научных статей по физике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a509753-7627-4bd2-a766-a9b36671345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06731de9354c43c89823c2aad59ea4fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса: 15000 статей\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../data/elibrary_physics_15k'\n",
    "article_id = re.compile(r'_([0-9]+).htm$')\n",
    "\n",
    "file_contents = []\n",
    "for filename in tqdm(os.scandir(data_directory), desc='Loading data'):\n",
    "    if filename.is_file():\n",
    "        file_name = filename.name\n",
    "        file_id = int(article_id.search(file_name).groups(1)[0])\n",
    "        \n",
    "        with open(filename, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            file_contents.append((file_id, text))\n",
    "\n",
    "print(f'Размер корпуса: {len(file_contents)} статей')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbedeae-a371-484e-8a17-b80bb518df45",
   "metadata": {},
   "source": [
    "Удалим из корпуса статьи, написанные не на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99885124-266f-4b89-9c72-998b035d98d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CYRILLIC_THRESHOLD = 0.2\n",
    "\n",
    "cyrillic_sym = re.compile(r'[а-яёА-ЯЁ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019181fb-21ac-4e06-8cab-a3c3e31dd666",
   "metadata": {},
   "source": [
    "Алгоритм отсеивания статей по языку:\n",
    "1. подсчитывается кол-во кириллических символов в статье;\n",
    "2. если соотношение кириллицы >= MIN_CYRILLIC_THRESHOLD -> статья русскоязычная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "256bb8e2-1323-43d3-b749-527627625ef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4914cab7bbe64aada93013364df9d3c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering out non-russian articles:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено нерусскоязычных статей: 4062\n"
     ]
    }
   ],
   "source": [
    "rus_articles = []\n",
    "\n",
    "for article in tqdm(file_contents, desc='Filtering out non-russian articles'):\n",
    "    rus_count = 0\n",
    "    for _ in cyrillic_sym.finditer(article[1]):\n",
    "        rus_count += 1\n",
    "\n",
    "    if rus_count >= len(article[1]) * MIN_CYRILLIC_THRESHOLD:\n",
    "        rus_articles.append(article)\n",
    "\n",
    "print(f'Удалено нерусскоязычных статей: {len(file_contents) - len(rus_articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8db5a258-0e19-40aa-8225-11362f3cd453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во русскоязычных статей: 10938\n"
     ]
    }
   ],
   "source": [
    "articles_count = len(rus_articles)\n",
    "print(f'Кол-во русскоязычных статей: {articles_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68ac2b6-a806-4a7c-840b-2ee2f25b3ff5",
   "metadata": {},
   "source": [
    "#### 1.2 Arranging lemmatizing procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebe52f-e84f-4576-8ae5-dc5b04502a5a",
   "metadata": {},
   "source": [
    "Напишем вспомогательные функции для анализа лемм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a14d3b05-7335-4fbe-9187-ce119355d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = pm2.MorphAnalyzer(lang='ru')\n",
    "\n",
    "pm2_cache = {}\n",
    "corpus_size = 6546389"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f59edb0-6f90-4400-b216-331e71a0d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bigram(word1_parse, word2_parse):\n",
    "    \"\"\"\n",
    "    Tries to match bigram of w1 and w2 with any of the patterns.\n",
    "    If succeed, returns a tuple ((w1_norm, w2_norm), pattern).\n",
    "    \"\"\"\n",
    "    word1_pos = word1_parse.tag.POS\n",
    "    word2_pos = word2_parse.tag.POS\n",
    "\n",
    "    # failed pattern matching\n",
    "    if word2_pos != 'NOUN' or word1_pos not in {'NOUN', 'ADJF'}:\n",
    "        return None\n",
    "\n",
    "    word1_inflection = (word1_parse.tag.gender, word1_parse.tag.number, word1_parse.tag.case)\n",
    "    word2_inflection = (word2_parse.tag.gender, word2_parse.tag.number, word2_parse.tag.case)\n",
    "\n",
    "    # 'adj + noun' pattern\n",
    "    if (\n",
    "        word1_pos == 'ADJF' and word2_pos == 'NOUN' \n",
    "        and word1_inflection[1] == word2_inflection[1]\n",
    "        and word1_inflection[2] == word2_inflection[2]\n",
    "        and (\n",
    "            word1_inflection[0] == word2_inflection[0]\n",
    "            or word1_inflection[0] is None and word1_inflection[1] == 'plur'\n",
    "        )\n",
    "    ):\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'прил. + сущ.'\n",
    "\n",
    "    # 'noun + noun' pattern\n",
    "    if word1_pos == 'NOUN' and word2_pos == 'NOUN' and word2_inflection[2] == 'gent':\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'сущ. + сущ.'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fac074-9e61-41fc-8c0a-dbc78463ffa3",
   "metadata": {},
   "source": [
    "Зададим список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70cb3d74-a6aa-410a-82c6-b22c16109fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "stop_words.extend(\n",
    "    [\n",
    "        # adjective-like pronouns\n",
    "        'мой', 'твой', 'ваш', 'наш', 'свой', 'его', 'ее', 'их',\n",
    "        'тот', 'этот', 'такой', 'таков', 'сей', 'который',\n",
    "        'весь', 'всякий', 'сам', 'самый', 'каждый', 'любой', 'иной', 'другой',\n",
    "        'какой', 'каков', 'чей', 'никакой', 'ничей',\n",
    "        'какой-то', 'какой-либо', 'какой-нибудь', 'некоторый', 'некий',\n",
    "        # participles\n",
    "        'соответствующий', 'следующий', 'данный',\n",
    "        # numerals\n",
    "        'один',\n",
    "        # insignificant words\n",
    "        'друг',\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4ae1fc-2499-4f41-8f41-6f3cd7d4e210",
   "metadata": {},
   "source": [
    "Зададим список стоп-биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c61ceb9-55ea-46ad-aa39-1cdb9124ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bigrams = []\n",
    "\n",
    "stop_bigrams.extend(\n",
    "    [\n",
    "        ('крайний', 'мера'), ('сегодняшний', 'день'), ('настоящий', 'время'), ('настоящий', 'работа'),\n",
    "        ('настоящий', 'статья'), ('точка', 'зрение'), ('первый', 'очередь'), ('последний', 'год'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_bigrams = set(stop_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3258bf-3370-4ec5-bdd9-c50f9774f7e3",
   "metadata": {},
   "source": [
    "Подходящие слова должны содержать только буквы и дефисы (последние не в начале/конце слова):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "695c708d-0c41-4dc5-95c9-bbc97a8ec775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_unigram(word):\n",
    "    \"\"\"Checks if a word is long enough and contains only letters and dashes.\"\"\"\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        return False\n",
    "\n",
    "    word_dashless = word.replace('-', '')\n",
    "    return len(word_dashless) >= 3 and word_dashless.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e872a16-4878-458b-ae63-bf0a85d311ab",
   "metadata": {},
   "source": [
    "Bottleneck лемматизации - обращения к pymorphy2. Сократим кол-во обращений к анализатору, кешируя морфологические разборы токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ededf681-933e-4404-b8b0-74299a1d428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_cache_token_parse(lemmatizer, token, cache):\n",
    "    \"\"\"Extracts token parse from cache, or adds it to the latter if not found.\"\"\"\n",
    "    if token in cache:\n",
    "        token_parse = cache[token]\n",
    "    else:\n",
    "        token_parse = lemmatizer.parse(token)[0]\n",
    "        cache[token] = token_parse\n",
    "\n",
    "    return token_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47be8dd9-f887-440f-a687-42919fe69e4b",
   "metadata": {},
   "source": [
    "Реализуем единый интерфейс сохранения tf для n-грамм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f607638d-bf31-4108-bebb-229ab37f667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tf_stats(n_gram_info, n_gram_collection, n):\n",
    "    \"\"\"Updates n-gram TF statistics with given n-gram.\"\"\"\n",
    "    # unigram\n",
    "    if n == 1:\n",
    "        if n_gram_info not in n_gram_collection:\n",
    "            n_gram_collection[n_gram_info] = {\n",
    "                'TF': 1,\n",
    "                'DF': 0,\n",
    "            }\n",
    "        else:\n",
    "            n_gram_collection[n_gram_info]['TF'] += 1\n",
    "    # bigram\n",
    "    elif n == 2:\n",
    "        n_gram, n_gram_pattern = n_gram_info\n",
    "        if n_gram not in n_gram_collection:\n",
    "            n_gram_collection[n_gram] = {\n",
    "                'Articles': [],\n",
    "                'Pattern': n_gram_pattern,\n",
    "                'TF': 1,\n",
    "                'DF': 0,\n",
    "            }\n",
    "        else:\n",
    "            n_gram_collection[n_gram]['TF'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e67b86-9d5b-46f4-b380-fc64082c5087",
   "metadata": {},
   "source": [
    "### 2. Forging Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c0c16-7ad9-41cc-be27-cfb8cf72382d",
   "metadata": {},
   "source": [
    "Сохранять датасет будем под следующим именем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5711fbbc-1e8e-40eb-9165-ff98ed7bdb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_full_name = '../data/data_frames/sentence_suitability_for_article.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c416cf4-bcad-4ce6-8612-95497f8fd258",
   "metadata": {},
   "source": [
    "Датасет вида **[[sentence, related_bigram, suitability], ...]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fd1d0b-df85-43de-86a7-32ee18958059",
   "metadata": {},
   "source": [
    "#### 2.1 Negative Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec818079-1056-474e-8c12-e8ddd88653a0",
   "metadata": {},
   "source": [
    "Для начала сформируем негативную часть выборки - предложения из корпуса elibrary, в которых не встретилось ни одного маркера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1a095b-1663-4f61-8243-58ad5133c1cd",
   "metadata": {},
   "source": [
    "Извлечем слова-маркеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a230c88-483b-473b-95af-33bb55c186f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_file = '../data/relation_markers/markers.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89abc2a6-dcb8-4fc2-a47b-636184999bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_markers = {}\n",
    "\n",
    "with open(markers_file, 'r') as f:\n",
    "    while (line := f.readline()) != '\\n':\n",
    "        line = line.strip()\n",
    "\n",
    "        # block headline\n",
    "        if line[0] == '*' and line[1] != '*':\n",
    "            cur_block = line[1:]\n",
    "            block_markers[cur_block] = []\n",
    "        # block separator\n",
    "        elif line[0] == '*' and line[1] == '*':\n",
    "            continue\n",
    "        # marker word\n",
    "        else:\n",
    "            block_markers[cur_block].append(line.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "652d0944-9c9a-4562-be57-48141c569015",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_set = []\n",
    "\n",
    "for markers in block_markers.values():\n",
    "    markers_set.extend(markers)\n",
    "\n",
    "markers_set = set(markers_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45468fa1-ea82-473a-b6b1-9cbaa6a29d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'больше',\n",
       " 'больший',\n",
       " 'в настоящее время',\n",
       " 'вариант',\n",
       " 'вид',\n",
       " 'включать',\n",
       " 'включить',\n",
       " 'входит в состав',\n",
       " 'давать возможность',\n",
       " 'дать возможность',\n",
       " 'известно',\n",
       " 'использование',\n",
       " 'использовать',\n",
       " 'использоваться',\n",
       " 'как правило',\n",
       " 'как принято',\n",
       " 'компонент',\n",
       " 'конструкция',\n",
       " 'меньше',\n",
       " 'меньший',\n",
       " 'модуль',\n",
       " 'недостаток',\n",
       " 'неэффективность',\n",
       " 'неэффективный',\n",
       " 'общеизвестно',\n",
       " 'общепринятый',\n",
       " 'обычно',\n",
       " 'означает',\n",
       " 'определение',\n",
       " 'определить',\n",
       " 'определять',\n",
       " 'особенность',\n",
       " 'отличие',\n",
       " 'отличительная особенность',\n",
       " 'по большей части',\n",
       " 'позволить',\n",
       " 'позволять',\n",
       " 'похож',\n",
       " 'предназначить',\n",
       " 'представлять',\n",
       " 'преимущество',\n",
       " 'применение',\n",
       " 'применить',\n",
       " 'применять',\n",
       " 'применяться',\n",
       " 'принадлежать',\n",
       " 'разновидность',\n",
       " 'следует считать',\n",
       " 'служить',\n",
       " 'содержат',\n",
       " 'состав',\n",
       " 'состоять',\n",
       " 'сравнение',\n",
       " 'сравнивать',\n",
       " 'сравнительный',\n",
       " 'сравнить',\n",
       " 'структура',\n",
       " 'сходство',\n",
       " 'считается',\n",
       " 'таким образом',\n",
       " 'термин',\n",
       " 'тип',\n",
       " 'традиционно',\n",
       " 'традиционный',\n",
       " 'удобный инструмент',\n",
       " 'часто',\n",
       " 'часть',\n",
       " 'чаще всего',\n",
       " 'элемент',\n",
       " 'это',\n",
       " 'эффективность',\n",
       " 'эффективный'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caaf9d7-32a6-46f1-b81e-a2648298f4fd",
   "metadata": {},
   "source": [
    "Загрузим датасет предложений с посчитанными метриками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ff0b491-b809-4249-ac7b-6b7661e48e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mi3_df = pd.read_csv('../data/data_frames/bigram_tfidf_mi3_scores_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2176f370-1abb-4b4e-9561-63a46c2b448e",
   "metadata": {},
   "source": [
    "Введем пороги, по которым будем отличать длинные предложения от средних и коротких:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5676416-2e13-4e37-9002-a899a33a4b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_SENT_THRESHOLD = 0.35\n",
    "MEDIUM_SENT_THRESHOLD = 0.5\n",
    "LONG_SENT_THRESHOLD = 0.7\n",
    "\n",
    "SHORT_SENT_MAX_LEN = 10\n",
    "MEDIUM_SENT_MAX_LEN = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a79afc-5b50-4b43-82b7-afac49fae5b9",
   "metadata": {},
   "source": [
    "Не будем учитывать предложения без \"сказуемого\" (без семантического анализа их редко когда можно точно определить):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af93463f-aa4b-4424-838a-a3f99aaecec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_predicate(lemmatizer, cache, tokens, predicates):\n",
    "    \"\"\"Checks if the sentence contains a predicate.\"\"\"\n",
    "    for token in tokens:\n",
    "        token_pos = get_or_cache_token_parse(lemmatizer, token, cache).tag.POS\n",
    "        if token_pos in predicates:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be0b869d-bd51-40a4-8e7e-702f45ab73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_definition(bigram, tokens):\n",
    "    \"\"\"Checks if the sentence has a structure <... bigram ... - ...>.\"\"\"\n",
    "    hyphens = ('-', '−', '–', '—')\n",
    "    for x in hyphens:\n",
    "        if x in tokens:\n",
    "            split_idx = tokens.index(x)\n",
    "            pre_define = tokens[:split_idx]\n",
    "\n",
    "            if bigram[0] in pre_define and bigram[1] in pre_define:\n",
    "                bigram_end_idx = tokens.index(bigram[1])\n",
    "\n",
    "                if split_idx - bigram_end_idx <= 5:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d844a88e-fb13-476c-bf3e-8a4dad3e792a",
   "metadata": {},
   "source": [
    "Сохраним подходящие контексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0c38a3c1-3197-4896-a3ce-ff3e6dbf4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_info = dict(rus_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28a5650c-8a68-4696-8648-8a948fd11213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330553fa7e7640ca9bf240a66cd3bc3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting bigram contexts:   0%|          | 0/36543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_desc_mi3 = bigram_mi3_df.to_dict('records')\n",
    "predicates = {'INFN', 'VERB', 'PRTS'}\n",
    "\n",
    "bigram_contexts = {}\n",
    "for bigram_info in tqdm(bigram_desc_mi3, desc='Extracting bigram contexts'):\n",
    "    unigram_1 = bigram_info['unigram_1']\n",
    "    unigram_2 = bigram_info['unigram_2']\n",
    "    bigram_name = f'{unigram_1}_{unigram_2}'\n",
    "    bigram_articles = eval(bigram_info['articles_id_list'])\n",
    "\n",
    "    # skipping all but top-10 frequent bigrams\n",
    "    if bigram_name not in top10_wikigrams:\n",
    "        continue\n",
    "\n",
    "    matching_sentences = []\n",
    "    for article_id in bigram_articles:\n",
    "        sentences = sent_tokenize(article_info[article_id], language='russian')\n",
    "\n",
    "        for sentence_idx, sentence in enumerate(sentences):\n",
    "            # sentence must start with a capital letter\n",
    "            if not sentence[0].isupper():\n",
    "                continue\n",
    "\n",
    "            tokens = word_tokenize(sentence.lower(), language='russian')\n",
    "        \n",
    "            standardized_tokens = []\n",
    "            for token in tokens:\n",
    "                if match_unigram(token):\n",
    "                    standardized_tokens.append(token)\n",
    "\n",
    "            for i in range(len(standardized_tokens) - 1):\n",
    "                token1, token2 = standardized_tokens[i], standardized_tokens[i + 1]\n",
    "        \n",
    "                token1_parse = get_or_cache_token_parse(lemmatizer, token1, pm2_cache)\n",
    "                token2_parse = get_or_cache_token_parse(lemmatizer, token2, pm2_cache)\n",
    "\n",
    "                # bigram must appear before the predicate\n",
    "                if token1_parse.tag.POS in predicates or token2_parse.tag.POS in predicates:\n",
    "                    break\n",
    "                \n",
    "                if token1_parse.normal_form in stop_words or token2_parse.normal_form in stop_words:\n",
    "                    continue\n",
    "        \n",
    "                bigram_data = match_bigram(token1_parse, token2_parse)\n",
    "                if bigram_data is not None:\n",
    "                    bigram, bigram_pattern = bigram_data\n",
    "        \n",
    "                    if bigram in stop_bigrams:\n",
    "                        continue\n",
    "\n",
    "                    if bigram == (unigram_1, unigram_2):\n",
    "                        sent_len = len(standardized_tokens)\n",
    "                        \n",
    "                        # sentence must contain a predicate or meet a definition template\n",
    "                        if (\n",
    "                            not has_predicate(lemmatizer, pm2_cache, standardized_tokens, predicates)\n",
    "                            and not is_definition((token1, token2), tokens)\n",
    "                        ):\n",
    "                            break\n",
    "\n",
    "                        # the closer bigram to the beginning of the sentence, the bigger its weight\n",
    "                        bigram_weight = 1 - i / sent_len\n",
    "                        \n",
    "                        # saving only those contexts where bigram appeared in the beginning\n",
    "                        if (\n",
    "                            sent_len <= SHORT_SENT_MAX_LEN and bigram_weight >= SHORT_SENT_THRESHOLD\n",
    "                            or sent_len > SHORT_SENT_MAX_LEN and sent_len <= MEDIUM_SENT_MAX_LEN and bigram_weight >= MEDIUM_SENT_THRESHOLD\n",
    "                            or sent_len > MEDIUM_SENT_MAX_LEN and bigram_weight >= LONG_SENT_THRESHOLD\n",
    "                        ):\n",
    "                            matching_sentences.append(sentence)\n",
    "\n",
    "                        # bigram found -> stop searching\n",
    "                        break\n",
    "\n",
    "    # saving extracted bigram contexts\n",
    "    if matching_sentences:\n",
    "        bigram_contexts[bigram_name] = matching_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4866b7-be1b-4911-b2fa-6044c35a25a5",
   "metadata": {},
   "source": [
    "Наложим ограничения на попадание в выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "92aef63b-3f0c-4add-b8fb-215141723be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suitability_status(sentence, formulae_pattern, good_symbols):\n",
    "    \"\"\"Checks all conditions to include the sentence in the dataset.\"\"\"\n",
    "    if (\n",
    "        # no equations\n",
    "        formulae_pattern.search(sentence) is None\n",
    "        # no incomplete fragments\n",
    "        and len(sentence) >= 10\n",
    "    ):\n",
    "        bad_symbols = set(sentence.lower()) - good_symbols\n",
    "\n",
    "        if not bad_symbols:\n",
    "            # remove the mess in the beginning\n",
    "            first_letter = re.search(r'[А-ЯA-Z]', sentence)\n",
    "            if first_letter is not None:\n",
    "                return first_letter.start()\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22030185-54a3-4498-8c0f-f05a1cc4df78",
   "metadata": {},
   "outputs": [],
   "source": [
    "formulae_pattern = re.compile(r'\\n.\\n.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39e704-fbd8-4f8f-b6a8-fd4aff813016",
   "metadata": {},
   "source": [
    "Отберем все символы, не встречающиеся в формулах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "09a0e5e0-bcbf-4cd6-92ba-6a25429323a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_symbols = {\n",
    "    'а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', \n",
    "    'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "    's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    '(', ')', '%', '\\n', ' ', ',', '.', '\"', '\\'', '!', '/', ':', ';', '?', '[', ']', '«', '»',\n",
    "    '³', '—', '–', '−', '…', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09fd4862-481b-40f6-8848-01b89debb574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'#',\n",
       " '*',\n",
       " '+',\n",
       " '-',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '\\\\',\n",
       " '^',\n",
       " '_',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~',\n",
       " '\\xad',\n",
       " '°',\n",
       " '±',\n",
       " '·',\n",
       " '×',\n",
       " '́',\n",
       " 'δ',\n",
       " 'ε',\n",
       " 'ζ',\n",
       " 'θ',\n",
       " 'λ',\n",
       " 'π',\n",
       " 'ρ',\n",
       " 'φ',\n",
       " '“',\n",
       " '„',\n",
       " '′',\n",
       " '\\u2061',\n",
       " 'ℎ',\n",
       " '→',\n",
       " '↔',\n",
       " '∀',\n",
       " '∂',\n",
       " '∇',\n",
       " '∈',\n",
       " '∑',\n",
       " '∗',\n",
       " '∘',\n",
       " '∞',\n",
       " '∫',\n",
       " '∮',\n",
       " '≈',\n",
       " '≠',\n",
       " '⋅',\n",
       " '⋯',\n",
       " '⟶',\n",
       " '⟺',\n",
       " '⩽',\n",
       " '⩾',\n",
       " '�'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # DEBUG\n",
    "\n",
    "# bad_symbols = set()\n",
    "\n",
    "# for filename in tqdm(os.scandir(raw_data_directory), desc='Finding bad symbols'):\n",
    "#     with open(filename, \"r\", encoding='utf-8') as f:\n",
    "#         sentences = sent_tokenize(f.read(), language='russian')\n",
    "\n",
    "#     for sentence in sentences:\n",
    "#         bad_symbols |= set(sentence.lower()) - good_symbols\n",
    "\n",
    "bad_symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8ac4e-052a-4966-a517-8a9f65d042cb",
   "metadata": {},
   "source": [
    "Пройдемся по всем предложениям биграмм и классифицируем их по наличию слов-маркеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5a1ac883-c0bc-415a-8651-59e1539a92fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd20e72d99745d481f7bc602e2dca4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filling dataset with negative samples:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "negative_samples = []\n",
    "\n",
    "for bigram, contexts in tqdm(bigram_contexts.items(), desc='Filling dataset with negative samples'):\n",
    "    for sentence in contexts:\n",
    "        is_matching = True\n",
    "\n",
    "        for marker in markers_set:\n",
    "            raw_sentence = sentence.lower()\n",
    "            \n",
    "            if raw_sentence.find(marker) != -1:\n",
    "                is_matching = False\n",
    "                break\n",
    "\n",
    "        if is_matching:\n",
    "            sentence_start = get_suitability_status(sentence, formulae_pattern, good_symbols)\n",
    "            if sentence_start != -1:\n",
    "                negative_samples.append([sentence[sentence_start:], bigram, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47048b83-b3ab-439f-8f6b-29bde954c183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2910"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negative_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c04058b-a2da-4b9a-8f24-113428551a3f",
   "metadata": {},
   "source": [
    "#### 2.2 Positive Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c0851-7f1a-4d38-98b0-2453a5b94e97",
   "metadata": {},
   "source": [
    "Соберем позитивную часть выборки - предложения из эталонных статей соответствующих биграмм на википедии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6c61334-801c-4f5d-9af7-3c33478e4063",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_directory = '../data/top10_wikigrams/wiki_data_raw'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db14daf8-d2e0-45c8-bd26-b5c409a2543a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a02e4f47f1d475cba2a553b86dce8ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning raw data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positive_samples = []\n",
    "\n",
    "for filename in tqdm(os.scandir(raw_data_directory), desc='Scanning raw data'):\n",
    "    with open(filename, \"r\", encoding='utf-8') as f:\n",
    "        sentences = sent_tokenize(f.read(), language='russian')\n",
    "\n",
    "    bigram = filename.name[:-4]\n",
    "    for sentence in sentences:\n",
    "        sentence_start = get_suitability_status(sentence, formulae_pattern, good_symbols)\n",
    "        if sentence_start != -1:\n",
    "            positive_samples.append([sentence[sentence_start:], bigram, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "141e83dd-7ac6-4644-97ad-67637da14c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf3b24-61f9-4592-b129-38e28f3f5bd2",
   "metadata": {},
   "source": [
    "#### 2.3 Merging Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea014ed-9136-4867-bf9d-fbe44db5b52f",
   "metadata": {},
   "source": [
    "Соединим негативные и позитивные примеры в 1 датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5810a740-dade-4cb5-a0d0-1440e3362e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = negative_samples + positive_samples\n",
    "\n",
    "ds_df = pd.DataFrame(dataset, columns=['text', 'related bigram', 'suitability'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cf3921f9-0499-4dcf-b845-7d5a33a9f06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>related bigram</th>\n",
       "      <th>suitability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Для нитрата никеля(II) зависимости изменения о...</td>\n",
       "      <td>длина_волна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>График зависимости инкремента от длины волны и...</td>\n",
       "      <td>длина_волна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Изменение отражения для каждой длины волны опр...</td>\n",
       "      <td>длина_волна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>На рис. 1а полоса поглощения на длине волны 58...</td>\n",
       "      <td>длина_волна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Детальнее ход этих зависимостей для поглощения...</td>\n",
       "      <td>длина_волна</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3453</th>\n",
       "      <td>Основными средствами коллективной защиты от во...</td>\n",
       "      <td>электрический_поле</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3454</th>\n",
       "      <td>Переносные и передвижные экранирующие устройст...</td>\n",
       "      <td>электрический_поле</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3455</th>\n",
       "      <td>В заземлённых кабинах и кузовах машин, механиз...</td>\n",
       "      <td>электрический_поле</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3456</th>\n",
       "      <td>This aura is the electric field due to the cha...</td>\n",
       "      <td>электрический_поле</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3457</th>\n",
       "      <td>The electric field is a vector field… and has ...</td>\n",
       "      <td>электрический_поле</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3458 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text      related bigram  \\\n",
       "0     Для нитрата никеля(II) зависимости изменения о...         длина_волна   \n",
       "1     График зависимости инкремента от длины волны и...         длина_волна   \n",
       "2     Изменение отражения для каждой длины волны опр...         длина_волна   \n",
       "3     На рис. 1а полоса поглощения на длине волны 58...         длина_волна   \n",
       "4     Детальнее ход этих зависимостей для поглощения...         длина_волна   \n",
       "...                                                 ...                 ...   \n",
       "3453  Основными средствами коллективной защиты от во...  электрический_поле   \n",
       "3454  Переносные и передвижные экранирующие устройст...  электрический_поле   \n",
       "3455  В заземлённых кабинах и кузовах машин, механиз...  электрический_поле   \n",
       "3456  This aura is the electric field due to the cha...  электрический_поле   \n",
       "3457  The electric field is a vector field… and has ...  электрический_поле   \n",
       "\n",
       "      suitability  \n",
       "0               0  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "...           ...  \n",
       "3453            1  \n",
       "3454            1  \n",
       "3455            1  \n",
       "3456            1  \n",
       "3457            1  \n",
       "\n",
       "[3458 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e01a8269-f056-4f5e-bfab-39ad52570491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df.to_csv(dataset_full_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970efb77-70dc-4206-82b1-1995e63d2792",
   "metadata": {},
   "source": [
    "### 3. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9233ee7c-3ff6-4ee7-ac7b-0b7b9f59cb2a",
   "metadata": {},
   "source": [
    "#### 3.1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d036f-e04e-4dc0-b483-dfa7dd775b35",
   "metadata": {},
   "source": [
    "- чистка от формул **нужна была в обеих частях** выборки;\n",
    "- выборка **сократилась почти в 2 раза** (пропорция 1 к 6 сохранилась), но некоторые формулы остались. Я убрал большую часть, остались безобидные по типу \"...область, в которой интенсивность выше порога многофотонной z fll...\". Т.е. это формулы, почти целиком состоящие из латинских букв. Удалять их не стал по 2 причинам: латинницей обозначаются физические величины, а они сплошь и рядом (получится слишком жесткая чистка), и в корпусе есть полноценные предложения на английском языке (их мало, но кто знает, останется ли это так при расширении выборки);\n",
    "- встречались хорошие, законченные предложения, **в начале которых была библиографическая информация** (ссылки), такие предварительно очищал от этой ненужной части;\n",
    "- к слову о библиографии: ссылки вида \"[1]\" **встречаются и внутри предложений**. Стоит ли регуляркой пройтись по всему корпусу, и удалить подобные фрагменты? Чтобы у моделей возникало минимум трудностей при дообучении."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90cc3d-4e42-421b-8ccf-426bd6bc77ae",
   "metadata": {},
   "source": [
    "#### 3.2 Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fc68e0-00b4-4fd4-98b6-8a7dc11ea2c5",
   "metadata": {},
   "source": [
    "- **TO-DO:**\n",
    "    1. ;\n",
    "    2. ;\n",
    "    3. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97bbb34-62a2-4dee-a6d1-3a622a1b4e97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

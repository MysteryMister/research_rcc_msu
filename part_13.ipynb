{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c1c4989-e27d-4a8b-ad7e-057157bf385d",
   "metadata": {},
   "source": [
    "# Output Ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706dd751-2e3a-424f-8413-3e3c6f6b528b",
   "metadata": {},
   "source": [
    "**Задача:**\n",
    "1. найти 20 редких терминов (10 со статьей на википедии и 10 без);\n",
    "2. получить для их текстов выдачу классификатора;\n",
    "3. добавить к выдаче доп. информацию (id статьи, № предложения, есть/нет маркер и тип маркера);\n",
    "4. отсортировать выдачу по этой информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "175af794-ae95-421a-8bdb-bd32529ca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import sentenize\n",
    "\n",
    "import pymorphy2 as pm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecfc71a4-4047-479a-ba3d-85a11f85c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import evaluate\n",
    "from datasets import ClassLabel, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pymorphy2 as pm2\n",
    "from razdel import sentenize\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from ipymarkup import show_span_box_markup\n",
    "from ipymarkup.palette import palette, RED, GREEN\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded99c08-f344-4473-bfc8-5d0ea702d2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MAX_SEQUENCE_LEN = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fdd3ba-bbb1-47a0-afa0-06d295bfffb8",
   "metadata": {},
   "source": [
    "### 0. Manual Term Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e079b9ba-0340-440d-8e18-5c79e01c941e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# terms with related wiki article\n",
    "wiki_terms = [\n",
    "    ('акустический', 'эмиссия'),\n",
    "    ('квантовый', 'точка'),\n",
    "    ('внутренний', 'волна'),\n",
    "    ('волна', 'рэлей'),\n",
    "    ('калибровочный', 'бозон'),\n",
    "    ('меченый', 'атом'),\n",
    "    ('синглётный', 'кислород'),\n",
    "    ('число', 'стокс'),\n",
    "    ('число', 'рэлей'),\n",
    "    ('тяжёлый', 'вода'),\n",
    "]\n",
    "\n",
    "# terms without article\n",
    "non_wiki_terms = [\n",
    "    ('волна', 'накачка'),\n",
    "    ('динамический', 'рекристаллизация'),\n",
    "    ('ионосферный', 'турбулентность'),\n",
    "    ('квантовый', 'биение'),\n",
    "    ('оптический', 'выпрямление'),\n",
    "    ('паразитный', 'мода'),\n",
    "    ('электронный', 'концентрация'),\n",
    "    ('хаотический', 'синхронизация'),\n",
    "    ('функция', 'память'),\n",
    "    ('ударный', 'слой'),\n",
    "]\n",
    "\n",
    "# combined terms\n",
    "terms = wiki_terms + non_wiki_terms\n",
    "\n",
    "len(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad75437-e68d-49ce-b60c-58092951a4e7",
   "metadata": {},
   "source": [
    "### 1. Data Crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa7325dd-4914-4aa9-8ddb-cfc4e7b5f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_cache_token_parse(lemmatizer, token, cache):\n",
    "    \"\"\"Extracts token parse from cache, or adds it to the latter if not found.\"\"\"\n",
    "    if token in cache:\n",
    "        token_parse = cache[token]\n",
    "    else:\n",
    "        token_parse = lemmatizer.parse(token)[0]\n",
    "        cache[token] = token_parse\n",
    "\n",
    "    return token_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dacd7e8-f05a-4098-9581-c511e34343d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = pm2.MorphAnalyzer(lang='ru')\n",
    "pm2_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628aebe0-a6fd-4c5e-85ab-1099c39463a9",
   "metadata": {},
   "source": [
    "Загрузим датасет - корпус научных статей по физике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94857d2c-0ae1-4d8a-b880-f106af7b6269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a646b3bded4318bf4d435985cbf16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса: 15000 статей\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../data/elibrary_physics_15k'\n",
    "article_id = re.compile(r'_([0-9]+).htm$')\n",
    "\n",
    "file_contents = {}\n",
    "for filename in tqdm(os.scandir(data_directory), desc='Loading data'):\n",
    "    if filename.is_file():\n",
    "        file_name = filename.name\n",
    "        file_id = int(article_id.search(file_name).groups(1)[0])\n",
    "        \n",
    "        with open(filename, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            file_contents[file_id] = text\n",
    "\n",
    "print(f'Размер корпуса: {len(file_contents)} статей')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c6c321-629b-4b4c-b26d-f693ede15c8f",
   "metadata": {},
   "source": [
    "Удалим из корпуса статьи, написанные не на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3426ab9e-41b6-4329-88fa-a1f9b0f33b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CYRILLIC_THRESHOLD = 0.2\n",
    "\n",
    "cyrillic_sym = re.compile(r'[а-яёА-ЯЁ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdff8573-f101-4cbe-ab64-214ec5029965",
   "metadata": {},
   "source": [
    "Алгоритм отсеивания статей по языку:\n",
    "1. подсчитывается кол-во кириллических символов в статье;\n",
    "2. если соотношение кириллицы >= MIN_CYRILLIC_THRESHOLD -> статья русскоязычная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977fdf23-19e9-4aff-aea4-bec94b161115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52984cfbe9e4ababdc1404f9b14c2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering out non-russian articles:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено нерусскоязычных статей: 4062\n"
     ]
    }
   ],
   "source": [
    "rus_articles = {}\n",
    "\n",
    "for id, article in tqdm(file_contents.items(), desc='Filtering out non-russian articles'):\n",
    "    rus_count = 0\n",
    "    for _ in cyrillic_sym.finditer(article):\n",
    "        rus_count += 1\n",
    "\n",
    "    if rus_count >= len(article) * MIN_CYRILLIC_THRESHOLD:\n",
    "        rus_articles[id] = article\n",
    "\n",
    "print(f'Удалено нерусскоязычных статей: {len(file_contents) - len(rus_articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "867cdd16-eb95-4206-a8ae-09a8abcd2541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во русскоязычных статей: 10938\n"
     ]
    }
   ],
   "source": [
    "articles_count = len(rus_articles)\n",
    "print(f'Кол-во русскоязычных статей: {articles_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602621e1-e760-42d7-be73-1fd76c6a4080",
   "metadata": {},
   "source": [
    "Напишем вспомогательные функции для анализа лемм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0face676-40c7-49f4-951f-4b6e4de2c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bigram(word1_parse, word2_parse):\n",
    "    \"\"\"\n",
    "    Tries to match bigram of w1 and w2 with any of the patterns.\n",
    "    If succeed, returns a tuple ((w1_norm, w2_norm), pattern).\n",
    "    \"\"\"\n",
    "    word1_pos = word1_parse.tag.POS\n",
    "    word2_pos = word2_parse.tag.POS\n",
    "\n",
    "    # failed pattern matching\n",
    "    if word2_pos != 'NOUN' or word1_pos not in {'NOUN', 'ADJF'}:\n",
    "        return None\n",
    "\n",
    "    word1_inflection = (word1_parse.tag.gender, word1_parse.tag.number, word1_parse.tag.case)\n",
    "    word2_inflection = (word2_parse.tag.gender, word2_parse.tag.number, word2_parse.tag.case)\n",
    "\n",
    "    # 'adj + noun' pattern\n",
    "    if (\n",
    "        word1_pos == 'ADJF' and word2_pos == 'NOUN' \n",
    "        and word1_inflection[1] == word2_inflection[1]\n",
    "        and word1_inflection[2] == word2_inflection[2]\n",
    "        and (\n",
    "            word1_inflection[0] == word2_inflection[0]\n",
    "            or word1_inflection[0] is None and word1_inflection[1] == 'plur'\n",
    "        )\n",
    "    ):\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'прил. + сущ.'\n",
    "\n",
    "    # 'noun + noun' pattern\n",
    "    if word1_pos == 'NOUN' and word2_pos == 'NOUN' and word2_inflection[2] == 'gent':\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'сущ. + сущ.'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2413864d-4e4a-4b3c-a13d-293362fe44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_bigram(bigram, lemmatizer, cache):\n",
    "    \"\"\"Simplifies bigram and returns its normal form.\"\"\"\n",
    "    unigram_1, unigram_2 = tuple(bigram.split(sep='_'))\n",
    "\n",
    "    normgram_1 = get_or_cache_token_parse(lemmatizer, unigram_1, cache).normal_form\n",
    "    normgram_2 = get_or_cache_token_parse(lemmatizer, unigram_2, cache).normal_form\n",
    "\n",
    "    return f'{normgram_1}_{normgram_2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aafe8ec-9459-482a-97f4-65f4d82c10df",
   "metadata": {},
   "source": [
    "Зададим список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49130ec8-1288-4d9c-a287-3da7e9dd84ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "stop_words.extend(\n",
    "    [\n",
    "        # adjective-like pronouns\n",
    "        'мой', 'твой', 'ваш', 'наш', 'свой', 'его', 'ее', 'их',\n",
    "        'тот', 'этот', 'такой', 'таков', 'сей', 'который',\n",
    "        'весь', 'всякий', 'сам', 'самый', 'каждый', 'любой', 'иной', 'другой',\n",
    "        'какой', 'каков', 'чей', 'никакой', 'ничей',\n",
    "        'какой-то', 'какой-либо', 'какой-нибудь', 'некоторый', 'некий',\n",
    "        # participles\n",
    "        'соответствующий', 'следующий', 'данный',\n",
    "        # numerals\n",
    "        'один',\n",
    "        # insignificant words\n",
    "        'друг',\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5c33b-abdd-4983-b0cc-536a2c9056bc",
   "metadata": {},
   "source": [
    "Зададим список стоп-биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff60d398-f697-498f-9af0-383f26e0c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bigrams = []\n",
    "\n",
    "stop_bigrams.extend(\n",
    "    [\n",
    "        ('крайний', 'мера'), ('сегодняшний', 'день'), ('настоящий', 'время'), ('настоящий', 'работа'),\n",
    "        ('настоящий', 'статья'), ('точка', 'зрение'), ('первый', 'очередь'), ('последний', 'год'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_bigrams = set(stop_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec192fe4-f110-4f17-ab0a-3adf669cbdf7",
   "metadata": {},
   "source": [
    "Подходящие слова должны содержать только буквы и дефисы (последние не в начале/конце слова):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7915a328-0c4e-4901-ba41-ded7f1fb2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_unigram(word):\n",
    "    \"\"\"Checks if a word is long enough and contains only letters and dashes.\"\"\"\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        return False\n",
    "\n",
    "    word_dashless = word.replace('-', '')\n",
    "    return len(word_dashless) >= 3 and word_dashless.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62ff3d9-9c73-457d-bfc6-377cc36b7bdb",
   "metadata": {},
   "source": [
    "Загрузим датасет предложений с посчитанными метриками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc1d3d4-fb2c-42bc-9484-05b53ceaa7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mi3_df = pd.read_csv('../data/data_frames/bigram_tfidf_mi3_scores_full.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2c78b19-bdc7-439b-a371-94b7f9a4344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_predicate(lemmatizer, cache, tokens, predicates):\n",
    "    \"\"\"Checks if the sentence contains a predicate.\"\"\"\n",
    "    for token in tokens:\n",
    "        token_pos = get_or_cache_token_parse(lemmatizer, token, cache).tag.POS\n",
    "        if token_pos in predicates:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786169e9-42f8-48cf-a4ba-11c747005aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_definition(bigram, tokens):\n",
    "    \"\"\"Checks if the sentence has a structure <... bigram ... - ...>.\"\"\"\n",
    "    hyphens = ('-', '−', '–', '—')\n",
    "    for x in hyphens:\n",
    "        if x in tokens:\n",
    "            split_idx = tokens.index(x)\n",
    "            pre_define = tokens[:split_idx]\n",
    "\n",
    "            if bigram[0] in pre_define and bigram[1] in pre_define:\n",
    "                bigram_end_idx = tokens.index(bigram[1])\n",
    "\n",
    "                if split_idx - bigram_end_idx <= 7:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1762b08-ce14-4a10-a47e-168769bc9077",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_hyphen(tokens):\n",
    "    \"\"\"Checks if the sentence has a hyphen and is valid.\"\"\"\n",
    "    valid_tokens_count = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if len(token) > 2:\n",
    "            valid_tokens_count += 1\n",
    "    if valid_tokens_count < 3:\n",
    "        return False\n",
    "\n",
    "    hyphens = ('-', '−', '–', '—')\n",
    "    for x in hyphens:\n",
    "        if x in tokens:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1472be70-1453-4db2-b880-7382d6b53816",
   "metadata": {},
   "source": [
    "Наложим ограничения на попадание в выборку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ed0d907-18a5-449a-a5b0-d9c95d9aa515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suitability_status(sentence, formulae_pattern, external_link, internal_link, good_symbols):\n",
    "    \"\"\"Checks all conditions to include the sentence in the dataset.\"\"\"\n",
    "    if (\n",
    "        # no equations\n",
    "        formulae_pattern.search(sentence) is None\n",
    "        # no external links\n",
    "        and external_link.search(sentence) is None\n",
    "        # no internal links\n",
    "        and internal_link.search(sentence) is None\n",
    "        # no incomplete fragments\n",
    "        and len(sentence) >= 10\n",
    "    ):\n",
    "        bad_symbols = set(sentence.lower()) - good_symbols\n",
    "\n",
    "        if not bad_symbols:\n",
    "            # remove the mess in the beginning\n",
    "            first_letter = re.search(r'[А-ЯA-Z]', sentence)\n",
    "            if first_letter is not None:\n",
    "                return first_letter.start()\n",
    "\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d84831de-3279-4292-b9a5-95f2a57bf8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle_pattern = re.compile('(==.*==(\\\\n)+)+')\n",
    "clumped_pattern = re.compile('[а-яА-Я0-9]\\.[а-яА-Я0-9]')\n",
    "\n",
    "formulae_pattern = re.compile(r'\\n.\\n.\\n')\n",
    "external_link_pattern = re.compile(r'\\[([0-9])+\\]')\n",
    "internal_link_pattern = re.compile(r'\\(([0-9])+\\)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a158e518-437c-4b0c-8053-1d087707fe52",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicates = {'INFN', 'VERB', 'PRTS', 'ADJS'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62338ce0-ce24-4134-acc1-00e6df60ab92",
   "metadata": {},
   "source": [
    "Отберем все символы, не встречающиеся в формулах:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2ff88f3-26a6-4472-bf18-b962cfb540c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_symbols = {\n",
    "    'а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', \n",
    "    'о', 'п', 'р', 'с', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r',\n",
    "    's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "    '(', ')', '%', '\\n', ' ', ',', '.', '\"', '\\'', '!', '/', ':', ';', '?', '[', ']', '«', '»',\n",
    "    '³', '—', '–', '−', '-', '…', \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d396a0-8e39-495a-bdc3-dc6aec99e759",
   "metadata": {},
   "source": [
    "Извлечем слова-маркеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79a6e4b1-aa4f-4a41-92c5-b700b33873c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_file = '../data/relation_markers/markers.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f782a49-53cc-4b3f-be2d-425a1b2c8c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_to_block = {}\n",
    "\n",
    "with open(markers_file, 'r') as f:\n",
    "    while (line := f.readline()) != '\\n':\n",
    "        line = line.strip()\n",
    "\n",
    "        # block headline\n",
    "        if line[0] == '*' and line[1] != '*':\n",
    "            cur_block = line[1:]\n",
    "        # block separator\n",
    "        elif line[0] == '*' and line[1] == '*':\n",
    "            continue\n",
    "        # marker word\n",
    "        else:\n",
    "            markers_to_block[line.lower()] = cur_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2729df07-27b4-4c6f-afcd-3c7c4595e374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'это': 'Определения',\n",
       " 'означает': 'Определения',\n",
       " 'вид': 'Определения',\n",
       " 'тип': 'Определения',\n",
       " 'разновидность': 'Определения',\n",
       " 'определить': 'Определения',\n",
       " 'определять': 'Определения',\n",
       " 'определение': 'Определения',\n",
       " 'вариант': 'Определения',\n",
       " 'представлять': 'Определения',\n",
       " 'термин': 'Определения',\n",
       " 'содержат': 'Структура',\n",
       " 'компонент': 'Структура',\n",
       " 'часть': 'Структура',\n",
       " 'входит в состав': 'Структура',\n",
       " 'структура': 'Структура',\n",
       " 'конструкция': 'Структура',\n",
       " 'включить': 'Структура',\n",
       " 'включать': 'Структура',\n",
       " 'состоять': 'Структура',\n",
       " 'модуль': 'Структура',\n",
       " 'элемент': 'Структура',\n",
       " 'принадлежать': 'Структура',\n",
       " 'состав': 'Структура',\n",
       " 'использоваться': 'Использование',\n",
       " 'использовать': 'Использование',\n",
       " 'использование': 'Использование',\n",
       " 'применение': 'Использование',\n",
       " 'применяться': 'Использование',\n",
       " 'позволить': 'Использование',\n",
       " 'позволять': 'Использование',\n",
       " 'давать возможность': 'Использование',\n",
       " 'дать возможность': 'Использование',\n",
       " 'удобный инструмент': 'Использование',\n",
       " 'предназначить': 'Использование',\n",
       " 'применить': 'Использование',\n",
       " 'применять': 'Использование',\n",
       " 'служить': 'Использование',\n",
       " 'особенность': 'Отличительные особенности',\n",
       " 'отличие': 'Отличительные особенности',\n",
       " 'преимущество': 'Отличительные особенности',\n",
       " 'недостаток': 'Отличительные особенности',\n",
       " 'отличительная особенность': 'Отличительные особенности',\n",
       " 'сходство': 'Отличительные особенности',\n",
       " 'похож': 'Отличительные особенности',\n",
       " 'больший': 'Отличительные особенности',\n",
       " 'меньший': 'Отличительные особенности',\n",
       " 'больше': 'Отличительные особенности',\n",
       " 'меньше': 'Отличительные особенности',\n",
       " 'сравнение': 'Отличительные особенности',\n",
       " 'сравнительный': 'Отличительные особенности',\n",
       " 'сравнить': 'Отличительные особенности',\n",
       " 'сравнивать': 'Отличительные особенности',\n",
       " 'эффективность': 'Отличительные особенности',\n",
       " 'эффективный': 'Отличительные особенности',\n",
       " 'неэффективный': 'Отличительные особенности',\n",
       " 'неэффективность': 'Отличительные особенности',\n",
       " 'обычно': 'Дополнительные сведения',\n",
       " 'известно': 'Дополнительные сведения',\n",
       " 'как правило': 'Дополнительные сведения',\n",
       " 'часто': 'Дополнительные сведения',\n",
       " 'чаще всего': 'Дополнительные сведения',\n",
       " 'по большей части': 'Дополнительные сведения',\n",
       " 'традиционно': 'Дополнительные сведения',\n",
       " 'как принято': 'Дополнительные сведения',\n",
       " 'общеизвестно': 'Дополнительные сведения',\n",
       " 'считается': 'Дополнительные сведения',\n",
       " 'следует считать': 'Дополнительные сведения',\n",
       " 'общепринятый': 'Дополнительные сведения',\n",
       " 'в настоящее время': 'Дополнительные сведения',\n",
       " 'таким образом': 'Дополнительные сведения',\n",
       " 'традиционный': 'Дополнительные сведения'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markers_to_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b52d2693-875b-4ec5-a50a-16131ed874b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_importance = {\n",
    "    'Определения': 1,\n",
    "    'Структура': 2,\n",
    "    'Использование': 3,\n",
    "    'Отличительные особенности': 4,\n",
    "    'Дополнительные сведения': 5,\n",
    "    None: 6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cf523eb-37f4-48aa-94e7-6ee6df94be23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text, text_idx, bigram):\n",
    "    \"\"\"Extracts classifiable sentences for target bigram from text.\"\"\"\n",
    "    sentences = []\n",
    "    for sentence_idx, substr in enumerate(sentenize(text)):\n",
    "        sentence = substr.text.strip(r'\\n')\n",
    "        \n",
    "        # removing subtitles\n",
    "        subtitle = subtitle_pattern.search(sentence)\n",
    "        if subtitle is not None:\n",
    "            sentence = sentence[:subtitle.start(0)] + sentence[subtitle.end(0):]\n",
    "        \n",
    "        # capital letter check\n",
    "        if not sentence[0].isupper():\n",
    "            continue\n",
    "        \n",
    "        # removing sentences with newline characters\n",
    "        if sentence.find('\\n') != -1:\n",
    "            continue\n",
    "\n",
    "        # removing clumped sentences\n",
    "        if clumped_pattern.search(sentence) is not None:\n",
    "            continue\n",
    "\n",
    "        # deemphasize sentence\n",
    "        sentence = sentence.replace('́', '')\n",
    "        \n",
    "        tokens = word_tokenize(sentence.lower(), language='russian')\n",
    "\n",
    "        # saving only sentences with target bigram\n",
    "        has_target_bigram = False\n",
    "        normalized_tokens = [get_or_cache_token_parse(lemmatizer, x, pm2_cache).normal_form for x in tokens]\n",
    "        for i in range(len(normalized_tokens) - 1):\n",
    "            if normalized_tokens[i] == bigram[0] and normalized_tokens[i + 1] == bigram[1]:\n",
    "                has_target_bigram = True\n",
    "                break\n",
    "        if not has_target_bigram:\n",
    "            continue\n",
    "        \n",
    "        # removing sentences without predicates\n",
    "        has_predicates = True\n",
    "        \n",
    "        standardized_tokens = []\n",
    "        for token in tokens:\n",
    "            if match_unigram(token):\n",
    "                standardized_tokens.append(token)\n",
    "\n",
    "        if (\n",
    "            not has_hyphen(tokens)\n",
    "            and not has_predicate(lemmatizer, pm2_cache, standardized_tokens, predicates)\n",
    "        ):\n",
    "            has_predicates = False\n",
    "\n",
    "        # removing formulas and links\n",
    "        sentence_start = get_suitability_status(sentence, formulae_pattern, external_link_pattern, internal_link_pattern, good_symbols)\n",
    "        if sentence_start != -1 and has_predicates:\n",
    "            final_sentence = sentence[sentence_start:]\n",
    "\n",
    "            # searching for markers\n",
    "            marker_type = None\n",
    "\n",
    "            # trying to find them in normal sentence\n",
    "            final_tokens = word_tokenize(final_sentence.lower(), language='russian')\n",
    "            token_set = set(final_tokens)\n",
    "            for marker, block in markers_to_block.items():\n",
    "                if marker in token_set:\n",
    "                    marker_type = block\n",
    "                    break\n",
    "\n",
    "            # checking the definition pattern\n",
    "            normalized_sentence = [get_or_cache_token_parse(lemmatizer, x, pm2_cache).normal_form for x in final_tokens]\n",
    "            if is_definition(bigram, normalized_sentence):\n",
    "                marker_type = 'Определения'\n",
    "\n",
    "            marker_importance = block_importance[marker_type]\n",
    "\n",
    "            sentences.append([final_sentence, text_idx, (sentence_idx + 1), marker_type, marker_importance])\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a3e3fd-3f44-43ea-bd26-d2d4a5442e4f",
   "metadata": {},
   "source": [
    "Сохраним подходящие контексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d543bda7-3e6d-4817-940e-4a74d9cc59c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9bb44ece924d92830b48a377f0242e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving matching contexts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles_df = {}\n",
    "\n",
    "for term in tqdm(terms, desc='Saving matching contexts'):\n",
    "    articles = bigram_mi3_df[(bigram_mi3_df['unigram_1'] == term[0]) & (bigram_mi3_df['unigram_2'] == term[1])]['articles_id_list']\n",
    "    articles = eval(list(articles.values)[0])\n",
    "\n",
    "    matched = []\n",
    "    for article_id in articles:\n",
    "        text = rus_articles[article_id]\n",
    "        matched.extend(extract_sentences(text, article_id, term))\n",
    "\n",
    "    df = pd.DataFrame(matched, columns=['sentence', 'article_id', 'sentence_number', 'marker_type', 'marker_importance'])\n",
    "    articles_df[f'{term[0]}_{term[1]}'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e9fb3ef9-82de-4914-86ad-35819dad555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, df in articles_df.items():\n",
    "    df.to_csv(f'../data/data_frames/eval_20/{term}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b926e6-7c4f-4702-bed2-09ba2e14c459",
   "metadata": {},
   "source": [
    "### 2. Sentence Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ea95310-478e-4cf5-876f-bb4e850a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"negative\", 1: \"positive\"}\n",
    "label2id = {\"negative\": 0, \"positive\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79d7341a-a4c9-42f5-89d5-b7a849ac51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../data/train_results/bert_pair/ruRoberta-large/outputs/checkpoint-4233'\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint_dir,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)\n",
    "\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer, device='cuda', batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6fe5f67-796b-44cb-82dc-59349bea85e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5265b2ea78f8454da0cd03aacc01a7b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding fitting sentences for term abstracts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 1100, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 943, in format\n",
      "    return fmt.format(record)\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 678, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\logging\\__init__.py\", line 368, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\traitlets\\config\\application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n",
      "    self.io_loop.start()\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 595, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\base_events.py\", line 1881, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Programs\\Python\\Python310\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n",
      "    await result\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\George\\AppData\\Local\\Temp\\ipykernel_3624\\223867874.py\", line 16, in <module>\n",
      "    for idx, output in enumerate(classifier(pair_generator)):\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\transformers\\pipelines\\text_classification.py\", line 156, in __call__\n",
      "    result = super().__call__(*inputs, **kwargs)\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py\", line 1167, in __call__\n",
      "    logger.warning_once(\n",
      "  File \"D:\\Dev\\science\\venv\\lib\\site-packages\\transformers\\utils\\logging.py\", line 329, in warning_once\n",
      "    self.warning(*args, **kwargs)\n",
      "Message: 'You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset'\n",
      "Arguments: (<class 'UserWarning'>,)\n"
     ]
    }
   ],
   "source": [
    "for term, df in tqdm(articles_df.items(), desc='Finding fitting sentences for term abstracts'):\n",
    "    # adding bigram info to dataframe\n",
    "    cleaned_term = term.replace('_', ' ')\n",
    "\n",
    "    df['text_pair'] = cleaned_term\n",
    "    df.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "    df = df.reindex(columns=['text', 'text_pair', 'article_id', 'sentence_number', 'marker_type', 'marker_importance'])\n",
    "\n",
    "    # preparing HF dataset\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    ds = ds.remove_columns(['article_id', 'sentence_number', 'marker_type', 'marker_importance'])\n",
    "\n",
    "    # classifying dataset items\n",
    "    pos_sentences = []\n",
    "    pair_generator = (pair for pair in ds)\n",
    "    for idx, output in enumerate(classifier(pair_generator)):\n",
    "        if output['label'] == 'positive':\n",
    "            text = ds[idx]['text']\n",
    "            score = output['score']\n",
    "\n",
    "            # restoring extra info\n",
    "            extra_info = list(df[df['text'] == text].values[0][2:])\n",
    "\n",
    "            pos_sentences.append([text] + extra_info + [score])\n",
    "\n",
    "    # sorting outputs by importance\n",
    "    sorted_sentences = sorted(pos_sentences, key=lambda x: (-x[4], x[5]), reverse=True)\n",
    "\n",
    "    # saving results\n",
    "    final_df = pd.DataFrame(sorted_sentences, columns=['sentence', 'article_id', 'sentence_number', 'marker_type', 'marker_importance', 'score'])\n",
    "    final_df.to_csv(f'../data/data_frames/eval_20_results/{term}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5cd07810-9400-4c53-8ed4-b07f43befd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "del model, classifier\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66843ba-e09b-47d3-a897-19591718718b",
   "metadata": {},
   "source": [
    "### 3. Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962d537d-57e7-4b0f-b430-22d094959f70",
   "metadata": {},
   "source": [
    "1. на некоторые термины без вики-статьи можно найти статьи с похожими названиями, но они о других явлениях;\n",
    "2. выдача по терминам неравномерная: кол-во найденных предложений варьируется в среднем от 3 до 10, но бывают и исключения (квантовая точка - 59);\n",
    "3. сортировал выдачу сначала по убыванию важности маркера (определения и структрура \"важнее\" доп. сведений), а после по убыванию оценки классификатора;\n",
    "4. маркеры не всегда адекватно структурируют выдачу - например, в определения проскакивают предложения, не определяющие свойства целевого термина, но содержащие слова-триггеры \"это\", \"является\" и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e251af7b-1a02-4be5-a7c5-620ab674a744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

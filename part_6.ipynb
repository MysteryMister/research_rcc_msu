{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc5bd528-daa5-459b-9c01-a9f0e34e569a",
   "metadata": {},
   "source": [
    "# Article Structure Emulation With Marked Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c6c349-38aa-4f73-a89c-3f3bc1f50cca",
   "metadata": {},
   "source": [
    "**Задача**: \n",
    "1. извлечь из списка маркировок ключевые слова по каждому блоку статьи;\n",
    "2. выделить для каждой подходящей биграммы предложения, содержащие хотя бы одно маркированное слово;\n",
    "3. упорядочить их в виде псевдо-статьи (по блокам)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10262f0a-88de-41d9-b266-9f99da7d985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2 as pm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3db51f-aa10-469d-b1e8-0062fe37b5c5",
   "metadata": {},
   "source": [
    "### 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9942e-24fd-4b42-9750-59270d7d7a64",
   "metadata": {},
   "source": [
    "#### 1.1 Loading corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9f282-c607-4317-924d-f48c238a26a2",
   "metadata": {},
   "source": [
    "Загрузим датасет - корпус научных статей по физике:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71b3604b-14aa-40b6-a982-ad0c321e8397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f776b0e590445a9de46684a4950565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading data: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер корпуса: 15000 статей\n"
     ]
    }
   ],
   "source": [
    "data_directory = '../data/elibrary_physics_15k'\n",
    "article_id = re.compile(r'_([0-9]+).htm$')\n",
    "\n",
    "file_contents = []\n",
    "for filename in tqdm(os.scandir(data_directory), desc='Loading data'):\n",
    "    if filename.is_file():\n",
    "        file_name = filename.name\n",
    "        file_id = int(article_id.search(file_name).groups(1)[0])\n",
    "        \n",
    "        with open(filename, \"r\", encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            file_contents.append((file_id, text))\n",
    "\n",
    "print(f'Размер корпуса: {len(file_contents)} статей')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d12ac-8d7f-415a-9831-9a24f2a3d5a4",
   "metadata": {},
   "source": [
    "#### 1.2 Filtering out non-russian articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9f70ed-4ab3-434f-9713-4748f274cc78",
   "metadata": {},
   "source": [
    "Удалим из корпуса статьи, написанные не на русском языке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f1cd5d-f80f-4d13-a7a7-6bad0226e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_CYRILLIC_THRESHOLD = 0.2\n",
    "\n",
    "cyrillic_sym = re.compile(r'[а-яёА-ЯЁ]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9b659-1baa-4a6f-b925-c8b8c8336e37",
   "metadata": {},
   "source": [
    "Алгоритм отсеивания статей по языку:\n",
    "1. подсчитывается кол-во кириллических символов в статье;\n",
    "2. если соотношение кириллицы >= MIN_CYRILLIC_THRESHOLD -> статья русскоязычная."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2478e5a0-dfb5-4c8f-a3e2-f1429307bd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10b82ffd6c8148a2aad28fdcd9a9c92c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering out non-russian articles:   0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Удалено нерусскоязычных статей: 4062\n"
     ]
    }
   ],
   "source": [
    "rus_articles = []\n",
    "\n",
    "for article in tqdm(file_contents, desc='Filtering out non-russian articles'):\n",
    "    rus_count = 0\n",
    "    for _ in cyrillic_sym.finditer(article[1]):\n",
    "        rus_count += 1\n",
    "\n",
    "    if rus_count >= len(article[1]) * MIN_CYRILLIC_THRESHOLD:\n",
    "        rus_articles.append(article)\n",
    "\n",
    "print(f'Удалено нерусскоязычных статей: {len(file_contents) - len(rus_articles)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4608e659-981f-448b-8683-3693822fa4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во русскоязычных статей: 10938\n"
     ]
    }
   ],
   "source": [
    "articles_count = len(rus_articles)\n",
    "print(f'Кол-во русскоязычных статей: {articles_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e6ed0-2000-45c5-91ad-7ac050eb81ce",
   "metadata": {},
   "source": [
    "#### 1.3 Arranging lemmatizing procedures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b041c8-401a-4265-a200-75f18db928e9",
   "metadata": {},
   "source": [
    "Напишем вспомогательные функции для анализа лемм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81a76169-ac37-44ee-8ab2-f9887cbe73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = pm2.MorphAnalyzer(lang='ru')\n",
    "pm2_cache = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5bd06df-5949-4576-9735-fa729d3fbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_bigram(word1_parse, word2_parse):\n",
    "    \"\"\"\n",
    "    Tries to match bigram of w1 and w2 with any of the patterns.\n",
    "    If succeed, returns a tuple ((w1_norm, w2_norm), pattern).\n",
    "    \"\"\"\n",
    "    word1_pos = word1_parse.tag.POS\n",
    "    word2_pos = word2_parse.tag.POS\n",
    "\n",
    "    # failed pattern matching\n",
    "    if word2_pos != 'NOUN' or word1_pos not in {'NOUN', 'ADJF'}:\n",
    "        return None\n",
    "\n",
    "    word1_inflection = (word1_parse.tag.gender, word1_parse.tag.number, word1_parse.tag.case)\n",
    "    word2_inflection = (word2_parse.tag.gender, word2_parse.tag.number, word2_parse.tag.case)\n",
    "\n",
    "    # 'adj + noun' pattern\n",
    "    if (\n",
    "        word1_pos == 'ADJF' and word2_pos == 'NOUN' \n",
    "        and word1_inflection[1] == word2_inflection[1]\n",
    "        and word1_inflection[2] == word2_inflection[2]\n",
    "        and (\n",
    "            word1_inflection[0] == word2_inflection[0]\n",
    "            or word1_inflection[0] is None and word1_inflection[1] == 'plur'\n",
    "        )\n",
    "    ):\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'прил. + сущ.'\n",
    "\n",
    "    # 'noun + noun' pattern\n",
    "    if word1_pos == 'NOUN' and word2_pos == 'NOUN' and word2_inflection[2] == 'gent':\n",
    "        return (word1_parse.normal_form, word2_parse.normal_form), 'сущ. + сущ.'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ddae8-a55d-4989-a954-ffbd6850d786",
   "metadata": {},
   "source": [
    "Зададим список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "929c76de-77d9-4690-abac-64d782f5323d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('russian')\n",
    "\n",
    "stop_words.extend(\n",
    "    [\n",
    "        # adjective-like pronouns\n",
    "        'мой', 'твой', 'ваш', 'наш', 'свой', 'его', 'ее', 'их',\n",
    "        'тот', 'этот', 'такой', 'таков', 'сей', 'который',\n",
    "        'весь', 'всякий', 'сам', 'самый', 'каждый', 'любой', 'иной', 'другой',\n",
    "        'какой', 'каков', 'чей', 'никакой', 'ничей',\n",
    "        'какой-то', 'какой-либо', 'какой-нибудь', 'некоторый', 'некий',\n",
    "        # participles\n",
    "        'соответствующий', 'следующий', 'данный',\n",
    "        # numerals\n",
    "        'один',\n",
    "        # insignificant words\n",
    "        'друг',\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_words = set(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220d64bd-717d-4b50-a45e-0a87fae02811",
   "metadata": {},
   "source": [
    "Зададим список стоп-биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "051f29a6-7079-4164-8cae-4d747784efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_bigrams = []\n",
    "\n",
    "stop_bigrams.extend(\n",
    "    [\n",
    "        ('крайний', 'мера'), ('сегодняшний', 'день'), ('настоящий', 'время'), ('настоящий', 'работа'),\n",
    "        ('настоящий', 'статья'), ('точка', 'зрение'), ('первый', 'очередь'), ('последний', 'год'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stop_bigrams = set(stop_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4426cf46-d04d-4548-ae01-3b4f7185d1b9",
   "metadata": {},
   "source": [
    "Подходящие слова должны содержать только буквы и дефисы (последние не в начале/конце слова):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b33d9bd6-d2be-409f-90f5-3aa51ab523c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_unigram(word):\n",
    "    \"\"\"Checks if a word is long enough and contains only letters and dashes.\"\"\"\n",
    "    if word.startswith('-') or word.endswith('-'):\n",
    "        return False\n",
    "\n",
    "    word_dashless = word.replace('-', '')\n",
    "    return len(word_dashless) >= 3 and word_dashless.isalpha()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd829fb-beae-4da8-896d-298e5dbe1741",
   "metadata": {},
   "source": [
    "Bottleneck лемматизации - обращения к pymorphy2. Сократим кол-во обращений к анализатору, кешируя морфологические разборы токенов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2889508d-38c2-41b2-83fa-8b9aae4cf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_cache_token_parse(lemmatizer, token, cache):\n",
    "    \"\"\"Extracts token parse from cache, or adds it to the latter if not found.\"\"\"\n",
    "    if token in cache:\n",
    "        token_parse = cache[token]\n",
    "    else:\n",
    "        token_parse = lemmatizer.parse(token)[0]\n",
    "        cache[token] = token_parse\n",
    "\n",
    "    return token_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24979c2-2070-4918-be99-9115f4be110a",
   "metadata": {},
   "source": [
    "### 2. Context Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5883d-ceb6-42df-8b62-ef06c8d7206e",
   "metadata": {},
   "source": [
    "Загрузим датасет предложений с посчитанными метриками:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30161672-8e3f-48b6-aecb-5020da962382",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mi3_df = pd.read_csv('../data/data_frames/bigram_tfidf_mi3_scores.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b287bc-704e-4d93-8129-e9ab240869f0",
   "metadata": {},
   "source": [
    "Введем пороги, по которым будем отличать длинные предложения от средних и коротких:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a34ff894-413c-4a2a-ae43-6cc5fa981ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHORT_SENT_THRESHOLD = 0.35\n",
    "MEDIUM_SENT_THRESHOLD = 0.5\n",
    "LONG_SENT_THRESHOLD = 0.7\n",
    "\n",
    "SHORT_SENT_MAX_LEN = 10\n",
    "MEDIUM_SENT_MAX_LEN = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0828b0-8686-4878-a861-e41195c51c2b",
   "metadata": {},
   "source": [
    "Не будем учитывать предложения без \"сказуемого\" (без семантического анализа их редко когда можно точно определить):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63cf4922-d02a-4042-aaa0-89635046e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_predicate(lemmatizer, cache, tokens, predicates):\n",
    "    \"\"\"Checks if the sentence contains a predicate.\"\"\"\n",
    "    for token in tokens:\n",
    "        token_pos = get_or_cache_token_parse(lemmatizer, token, cache).tag.POS\n",
    "        if token_pos in predicates:\n",
    "            return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e18b3e9b-780c-4d11-9ab6-0c71b165cec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_definition(bigram, tokens):\n",
    "    \"\"\"Checks if the sentence has a structure <... bigram ... - ...>.\"\"\"\n",
    "    hyphens = ('-', '−', '–', '—')\n",
    "    for x in hyphens:\n",
    "        if x in tokens:\n",
    "            split_idx = tokens.index(x)\n",
    "            pre_define = tokens[:split_idx]\n",
    "\n",
    "            if bigram[0] in pre_define and bigram[1] in pre_define:\n",
    "                bigram_end_idx = tokens.index(bigram[1])\n",
    "\n",
    "                if split_idx - bigram_end_idx <= 5:\n",
    "                    return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9256a4a3-4fe0-4fec-a26c-badffc291ca5",
   "metadata": {},
   "source": [
    "Извлечем предложения-контексты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c467e24-694b-46bf-96c6-c192301e1c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_info = dict(rus_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d93011a5-0b7f-40ec-ae54-f85549e53d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2921cc43164fc48fee5b3603cd1ffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting bigram contexts:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bigram_desc_mi3 = bigram_mi3_df.to_dict('records')\n",
    "predicates = {'INFN', 'VERB', 'PRTS'}\n",
    "\n",
    "bigram_contexts = {}\n",
    "for bigram_info in tqdm(bigram_desc_mi3, desc='Extracting bigram contexts'):\n",
    "    unigram_1 = bigram_info['unigram_1']\n",
    "    unigram_2 = bigram_info['unigram_2']\n",
    "    bigram_name = f'{unigram_1}_{unigram_2}'\n",
    "    bigram_articles = eval(bigram_info['articles_id_list'])\n",
    "\n",
    "    matching_sentences = []\n",
    "    for article_id in bigram_articles:\n",
    "        sentences = sent_tokenize(article_info[article_id], language='russian')\n",
    "\n",
    "        for sentence_idx, sentence in enumerate(sentences):\n",
    "            # sentence must start with a capital letter\n",
    "            if not sentence[0].isupper():\n",
    "                continue\n",
    "\n",
    "            tokens = word_tokenize(sentence.lower(), language='russian')\n",
    "        \n",
    "            standardized_tokens = []\n",
    "            for token in tokens:\n",
    "                if match_unigram(token):\n",
    "                    standardized_tokens.append(token)\n",
    "\n",
    "            for i in range(len(standardized_tokens) - 1):\n",
    "                token1, token2 = standardized_tokens[i], standardized_tokens[i + 1]\n",
    "        \n",
    "                token1_parse = get_or_cache_token_parse(lemmatizer, token1, pm2_cache)\n",
    "                token2_parse = get_or_cache_token_parse(lemmatizer, token2, pm2_cache)\n",
    "\n",
    "                # bigram must appear before the predicate\n",
    "                if token1_parse.tag.POS in predicates or token2_parse.tag.POS in predicates:\n",
    "                    break\n",
    "                \n",
    "                if token1_parse.normal_form in stop_words or token2_parse.normal_form in stop_words:\n",
    "                    continue\n",
    "        \n",
    "                bigram_data = match_bigram(token1_parse, token2_parse)\n",
    "                if bigram_data is not None:\n",
    "                    bigram, bigram_pattern = bigram_data\n",
    "        \n",
    "                    if bigram in stop_bigrams:\n",
    "                        continue\n",
    "\n",
    "                    if bigram == (unigram_1, unigram_2):\n",
    "                        sent_len = len(standardized_tokens)\n",
    "                        \n",
    "                        # sentence must contain a predicate or meet a definition template\n",
    "                        if (\n",
    "                            not has_predicate(lemmatizer, pm2_cache, standardized_tokens, predicates)\n",
    "                            and not is_definition((token1, token2), tokens)\n",
    "                        ):\n",
    "                            break\n",
    "\n",
    "                        # the closer bigram to the beginning of the sentence, the bigger its weight\n",
    "                        bigram_weight = 1 - i / sent_len\n",
    "                        \n",
    "                        # saving only those contexts where bigram appeared in the beginning\n",
    "                        if (\n",
    "                            sent_len <= SHORT_SENT_MAX_LEN and bigram_weight >= SHORT_SENT_THRESHOLD\n",
    "                            or sent_len > SHORT_SENT_MAX_LEN and sent_len <= MEDIUM_SENT_MAX_LEN and bigram_weight >= MEDIUM_SENT_THRESHOLD\n",
    "                            or sent_len > MEDIUM_SENT_MAX_LEN and bigram_weight >= LONG_SENT_THRESHOLD\n",
    "                        ):\n",
    "                            matching_sentences.append(sentence)\n",
    "\n",
    "                        # bigram found -> stop searching\n",
    "                        break\n",
    "\n",
    "    # saving extracted bigram contexts\n",
    "    if matching_sentences:\n",
    "        bigram_contexts[bigram_name] = matching_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5024787c-8a5a-4dd8-bb5a-329fe8fc368f",
   "metadata": {},
   "source": [
    "### 3. Forming Article Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddde855-a342-4e04-9006-c2cfa0361e0d",
   "metadata": {},
   "source": [
    "Построим скелет статей по каждой подходящей биграмме."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e03bc-eda6-49aa-abc9-567ef750c8b7",
   "metadata": {},
   "source": [
    "#### 3.1 Markings Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7b1a5-d4d6-42ce-99f4-24fbf8e02c4b",
   "metadata": {},
   "source": [
    "Для начала, извлечем слова-маркеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39769bb8-6c07-4d50-b899-d2480d91b5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers_file = '../data/relation_markers/markers.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b149481-f780-49ea-93ed-152aa1e23d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_markers = {}\n",
    "\n",
    "with open(markers_file, 'r') as f:\n",
    "    while (line := f.readline()) != '\\n':\n",
    "        line = line.strip()\n",
    "\n",
    "        # block headline\n",
    "        if line[0] == '*' and line[1] != '*':\n",
    "            cur_block = line[1:]\n",
    "            block_markers[cur_block] = []\n",
    "        # block separator\n",
    "        elif line[0] == '*' and line[1] == '*':\n",
    "            continue\n",
    "        # marker word\n",
    "        else:\n",
    "            block_markers[cur_block].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d97d456-9851-4b9f-8c85-88061dd4fafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['содержат',\n",
       " 'компонент',\n",
       " 'часть',\n",
       " 'входит в состав',\n",
       " 'структура',\n",
       " 'конструкция',\n",
       " 'включить',\n",
       " 'включать',\n",
       " 'состоять',\n",
       " 'модуль',\n",
       " 'элемент',\n",
       " 'принадлежать',\n",
       " 'состав',\n",
       " 'структура']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_markers['Структура']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb3bb57-738f-4da8-bf79-8171774348e2",
   "metadata": {},
   "source": [
    "#### 3.2 Sentence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0f936c-bc71-4b36-abd5-6f08603d5ab8",
   "metadata": {},
   "source": [
    "Пройдемся по всем предложениям биграмм и классифицируем их по наличию слов-маркеров:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "714ee284-e093-4b8d-954b-7cf48c08bfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_directory = '../data/article_skeletons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7bd07e4-8795-490d-b0ad-1fc71ff2460a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59001fb2af2e426397c71b8768c3d32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating article skeletons:   0%|          | 0/208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for bigram, contexts in tqdm(bigram_contexts.items(), desc='Creating article skeletons'):\n",
    "    # classifying sentences\n",
    "    topic_sentences = defaultdict(list)\n",
    "\n",
    "    for sentence in contexts:\n",
    "        is_included = False\n",
    "        tokens = set(word_tokenize(sentence.lower(), language='russian'))\n",
    "\n",
    "        for topic, markers in block_markers.items():\n",
    "            for marker in markers:\n",
    "                if marker in tokens:\n",
    "                    topic_sentences[topic].append(sentence)\n",
    "                    is_included = True\n",
    "                    break\n",
    "\n",
    "            if is_included:\n",
    "                break\n",
    "\n",
    "    is_empty_skeleton = True\n",
    "    for content in topic_sentences.values():\n",
    "        if content:\n",
    "            is_empty_skeleton = False\n",
    "            break\n",
    "\n",
    "    # saving only non-empty results\n",
    "    if not is_empty_skeleton:\n",
    "        file_name = f'{result_directory}/{bigram}.txt'\n",
    "        with open(file_name, 'w') as f:\n",
    "            f.write(f'{bigram} — термин физики.\\n')\n",
    "\n",
    "            for topic in block_markers:\n",
    "                sentences = topic_sentences[topic]\n",
    "\n",
    "                if sentences:\n",
    "                    f.write('\\n')\n",
    "                    f.write(f'{topic}\\n')\n",
    "                    f.write('*' * 40)\n",
    "                    f.write('\\n')\n",
    "\n",
    "                    for sentence in sentences:\n",
    "                        f.write(sentence)\n",
    "                        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5c3ce9-a2da-4fef-8371-634356e02a6f",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f1c493-2d56-4e32-8795-5a34903006de",
   "metadata": {},
   "source": [
    "#### 4.1 Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca2c5c-1280-427e-b993-98e127d59c20",
   "metadata": {},
   "source": [
    "- **наихудшее качество** классификации: **определения** и **структура**. В случае определений, слово \"это\" особенно больно портит картину, т.к. чаще всего оно употребляется в контекстах \"это связано с...\", \"это обстоятельство помогло...\" (т.е. это как местоимение-прилагательное). Кажется, маркеров отдельных слов/пар недостаточно для +- адекатной генерации этих 2 блоков - тут могут пригодиться маркеры-шаблоны (\"В данной работе рассматривается ...\", \"... - это ...\", т.е. целые синтаксические конструкции как маркеры);\n",
    "- **наилучшее качество** классификации: **дополнительные сведения**. На удивление, относительно часто выдается неплохая доп. справка по термину, т.е. этот блок более-менее можно составлять чисто на маркерах.\n",
    "- поскольку каждое предложение учитывал **ровно 1 раз**, то логично, что в первые категории (определения и структура) **попадает большинство** (возможно, и это тоже добаляет шума в выдачу). Возможно, в предложении описывалось использование термина, но из-за слова \"это\" оно неправильно попало в определения (просто потому что первыми проверялись маркеры именно этого блока). Эту проблему не решить без семантического анализа контекста;\n",
    "- сами биграммы при записи в 1 строку файлов пока оставлял в виде \"норм-форма_норм-форма\", если нужно точно восстановить согласованность, то сделаю (правда, это не так просто);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75278e96-c4d7-42c8-be4c-a5de824923f9",
   "metadata": {},
   "source": [
    "#### 4.2 Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf623e-1839-470f-afb4-4123db3c1a42",
   "metadata": {},
   "source": [
    "- **TO-DO:**\n",
    "    1. ;\n",
    "    2. ;\n",
    "    3. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25374b35-1d7a-45d0-a36c-83bc771d8b19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science",
   "language": "python",
   "name": "science"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
